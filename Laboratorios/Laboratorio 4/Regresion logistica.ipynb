{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Laboratorio 4(Aplicacion de Regularizacion - Regresion Logistica) Grupo 1\n",
    " <h3>En este laboratorio se hizo el uso del dataset para aplicar la regularización y sin utilizar regularización a la Regresion Logistica, y como siguiente se presento los resultados de ambas experiencias<h3>\n",
    " <HR>\n",
    " <h3>\n",
    "  NOMBRE: POLO ORELLANA BRAYAN SIMON <br>\n",
    "  CARRERA: INGENIERIA DE SISTEMAS <BR>\n",
    "  FECHA: 02/04/2024 <BR>\n",
    "\n",
    "  * [Enlace de invitacion para ser colaborador](https://github.com/bspoloo/SIS420-012024/invitations)\n",
    "  \n",
    "  * [Enlace al git hub](https://github.com/bspoloo/SIS420-012024/tree/main/Laboratorios/Laboratorio%204)\n",
    "  \n",
    "  * [Enlace al Colab](https://colab.research.google.com/github/bspoloo/SIS420-012024/blob/main/Laboratorios/Laboratorio%204/Regresion%20logistica.ipynb?hl=es)\n",
    " <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el laboratorio hizo uso del para entrenar el modelo de **Regresion Logistica** aplicado regularizacion y predecir si un paciente tiene alguna enfermedad cardiovascular.\n",
    "\n",
    "Estos problemas a menudo se deben a la aterosclerosis. Esta afección ocurre cuando la grasa y el colesterol se acumulan en las paredes del vaso sanguíneo (arteria). El enlace al dataset es [Cardiovascular Disease dataset](https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset).\n",
    "\n",
    "El archivo `cardiovascular_diseases_dv3.csv` contiene un conjunto de datos de entrenamiento de datos si un paciente tiene una enfermedad cardiovascular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se importo todas las librerias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando la libreria os para manejos de directorios\n",
    "import os\n",
    "\n",
    "# Computacion vectorial y cientifica para python\n",
    "import numpy as np\n",
    "\n",
    "#importamos pandas para el manejo del dataset, y separarlos dentro de una matriz\n",
    "import pandas as pd\n",
    "\n",
    "#esta tabulate nos sirve para hacer tablas\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Librerias para graficación (trazado de gráficos)\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # -> Necesario para graficar superficies 3D\n",
    "\n",
    "#Para separa el 20% y 80% de los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# llama a matplotlib a embeber graficas dentro de los cuadernillos\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos del dataset\n",
    "\n",
    "cargamos los datos haciendo el uso de la libreria **Pandas** que  es una herramienta poderosa y versátil utilizada para manipulación y análisis de datos. Ofrece estructuras de datos flexibles y eficientes para trabajar con datos tabulares, como hojas de cálculo en Excel o tablas SQL. Algunas de las funcionalidades clave de pandas incluyen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>AP_HIGH</th>\n",
       "      <th>AP_LOW</th>\n",
       "      <th>CHOLESTEROL</th>\n",
       "      <th>GLUCOSE</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>ALCOHOL</th>\n",
       "      <th>PHYSICAL_ACTIVITY</th>\n",
       "      <th>CARDIO_DISEASE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68778</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68779</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68780</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68781</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68782</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68783 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGE  GENDER  HEIGHT  WEIGHT  AP_HIGH  AP_LOW  CHOLESTEROL  GLUCOSE  \\\n",
       "0       50       2     168      62      110      80            1        1   \n",
       "1       55       1     156      85      140      90            3        1   \n",
       "2       52       1     165      64      130      70            3        1   \n",
       "3       48       2     169      82      150     100            1        1   \n",
       "4       48       1     156      56      100      60            1        1   \n",
       "...    ...     ...     ...     ...      ...     ...          ...      ...   \n",
       "68778   53       2     168      76      120      80            1        1   \n",
       "68779   62       1     158     126      140      90            2        2   \n",
       "68780   52       2     183     105      180      90            3        1   \n",
       "68781   61       1     163      72      135      80            1        2   \n",
       "68782   56       1     170      72      120      80            2        1   \n",
       "\n",
       "       SMOKE  ALCOHOL  PHYSICAL_ACTIVITY  CARDIO_DISEASE  \n",
       "0          0        0                  1               0  \n",
       "1          0        0                  1               1  \n",
       "2          0        0                  0               1  \n",
       "3          0        0                  1               1  \n",
       "4          0        0                  0               0  \n",
       "...      ...      ...                ...             ...  \n",
       "68778      1        0                  1               0  \n",
       "68779      0        0                  1               1  \n",
       "68780      0        1                  0               1  \n",
       "68781      0        0                  0               1  \n",
       "68782      0        0                  1               0  \n",
       "\n",
       "[68783 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cargamos el dataset a un dataframe de pandas\n",
    "df = pd.read_csv('cardiovascular_diseases_dv3.csv', delimiter=';')\n",
    "\n",
    "#mostramos el dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis del dataset\n",
    "Hacemos un analisis del dataset mostrando su informacion usando la funcion de `info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68783 entries, 0 to 68782\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype\n",
      "---  ------             --------------  -----\n",
      " 0   AGE                68783 non-null  int64\n",
      " 1   GENDER             68783 non-null  int64\n",
      " 2   HEIGHT             68783 non-null  int64\n",
      " 3   WEIGHT             68783 non-null  int64\n",
      " 4   AP_HIGH            68783 non-null  int64\n",
      " 5   AP_LOW             68783 non-null  int64\n",
      " 6   CHOLESTEROL        68783 non-null  int64\n",
      " 7   GLUCOSE            68783 non-null  int64\n",
      " 8   SMOKE              68783 non-null  int64\n",
      " 9   ALCOHOL            68783 non-null  int64\n",
      " 10  PHYSICAL_ACTIVITY  68783 non-null  int64\n",
      " 11  CARDIO_DISEASE     68783 non-null  int64\n",
      "dtypes: int64(12)\n",
      "memory usage: 6.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con la siguiente informacion se puede ver lo siguiente, el cual son las caracteristicas del dataset:\n",
    "\n",
    "**Variables de entrada(X):**\n",
    "\n",
    "1. **age(Edad).-** Se refiere a la edad del paciente en años.\n",
    "2. **gender(Genero).-** Hace referencia al genero del paciente, 1 si es hombre y 2 si es mujer.\n",
    "3. **height(Altura).-** Es la altura del paciente medido en cm.\n",
    "4. **weight(Peso).-** Es el peso del paciente medido en Kg\n",
    "5. **ap_hight(Presión arterial sistólica).-** Presión arterial sistólica del paciente.\n",
    "6. **ap_low(Presión arterial diastólica).-** Presión arterial diastólica del paciente.\n",
    "7. **cholesterol(Colesterol).-** Nivel de colesterol en el paciente donde. 1: normal, 2: por encima de lo normal, 3: muy por encima de lo normal.\n",
    "8. **glocose(Glucosa).-** Nivel de glucosa del paciente donde. 1: normal, 2: por encima de lo normal, 3: muy por encima de lo normal.\n",
    "9. **smoke(Fuma).-** Indica si el paciente fuma. 1:Si y 2:No.\n",
    "10. **alcohol(alcohol).-** Indica si el paciente consume alcohol. 1:Si y 2:No.\n",
    "11. **fhisycal activity(Actividad fisica).-** Indical si el paciente realiza algun tipo de actividad fisica donde. 1:Si y 2:No.\n",
    "\n",
    "**Variable de salida(y):**\n",
    "\n",
    "1. **cardiovascular disease(Enfermedad cardiovascular).-** Presencia o ausencia de enfermedad cardiovascular, donde: 1:Si y 2:No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion del 80% de los datos para entrenamiento y 20% para pruebas\n",
    "\n",
    "Haremos uso de la libreria `sklearn` haciendo uso de su funcion `train_test_split()`, donde recibe como parametros:\n",
    "\n",
    "`Arrays o matrices de características (X): `Estos son los datos que se utilizarán para hacer predicciones. Por lo general, son las variables independientes o características del conjunto de datos.\n",
    "\n",
    "`test_size (opcional):` Este parámetro especifica el tamaño del conjunto de prueba. Puede ser un número decimal entre 0 y 1, que representa el porcentaje del conjunto de datos que se asignará al conjunto de prueba, o puede ser un entero que representa el número absoluto de muestras en el conjunto de prueba. Por ejemplo, si test_size=0.2, se asignará el 20% del conjunto de datos al conjunto de prueba.\n",
    "\n",
    "`train_size (opcional):` Este parámetro especifica el tamaño del conjunto de entrenamiento. Al igual que test_size, puede ser un número decimal entre 0 y 1 o un entero que representa el número absoluto de muestras en el conjunto de entrenamiento. Si no se proporciona, se calcula automáticamente como 1 - test_size.\n",
    "\n",
    "`random_state (opcional):` Este parámetro permite establecer una semilla para la generación de números pseudoaleatorios. Esto garantiza que la división de los datos sea reproducible. Si se establece en un número entero, el resultado será el mismo cada vez que se ejecute el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]         Y\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "      50       1       173        72     120      70       1       1       0         0       1           1\n",
      "      62       2       174        52     120      80       1       1       1         0       1           1\n",
      "      46       2       156        62     115      70       1       1       0         0       1           0\n",
      "      58       1       158        90     140      90       1       1       0         0       1           1\n",
      "      62       1       156        90     160      80       3       3       0         0       1           0\n",
      "      60       1       151        44     120      80       1       2       0         0       1           0\n",
      "      40       1       156        56     120      70       1       1       0         0       1           0\n",
      "      45       1       157        43     120      80       1       1       0         0       1           0\n",
      "      56       1       167        78     130      80       3       3       0         0       1           1\n",
      "      40       2       174        82     120      80       1       1       1         0       1           0\n",
      " \n",
      "El 80% de ejemplos para entrenamiento son la cantidad de: 55026 de ejemplos\n",
      "El 20% de ejemplos para pruebas son la cantidad de: 13757 de ejemplos\n",
      "La cantidad total de ejemplos es de: 68783 de ejemplos\n"
     ]
    }
   ],
   "source": [
    "# usamos la libreria train_test_split que nos ayudara a separar el 80% y 20% de los datos.\n",
    "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#Separamos en X_test los datos, pero dropeando nuestra y que seria 'CARDIO_DISEASE'\n",
    "X_test = test_dataset.drop(['CARDIO_DISEASE'], axis=1).values\n",
    "#Separamos en y_test los datos, pero solo cargando la columna de 'CARDIO_DISEASE', ya que esa sera nuestra y\n",
    "y_test = test_dataset['CARDIO_DISEASE'].values\n",
    "m_test = len(y_test)\n",
    "\n",
    "# tomamos train_dataset, seleccionamos las columnas para X_train y la columna 'CARDIO_DISEASE' para y_train\n",
    "X_train = train_dataset.drop(['CARDIO_DISEASE'], axis=1).values\n",
    "y_train = train_dataset['CARDIO_DISEASE'].values\n",
    "m_train = len(y_train)\n",
    "\n",
    "\n",
    "#Imprimimos algunos datos:\n",
    "# imprimir todos las X de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'Y'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}{:10.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:10.0f}{:8.0f}{:12.0f}'.format(\n",
    "        X_train[i, 0], X_train[i, 1], X_train[i, 2], X_train[i, 3], X_train[i, 4], X_train[i, 5], X_train[i, 6], X_train[i, 7], X_train[i, 8], X_train[i, 9], X_train[i, 10], y_train[i]\n",
    "    ))\n",
    "\n",
    "print(\" \")\n",
    "print('El 80% de ejemplos para entrenamiento son la cantidad de: {:.0f} de ejemplos'.format( len(train_dataset)))\n",
    "print('El 20% de ejemplos para pruebas son la cantidad de: {:.0f} de ejemplos'.format( len(test_dataset)))\n",
    "print('La cantidad total de ejemplos es de: {:.0f} de ejemplos'.format( len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion para la Normalización de caracteristicas\n",
    "\n",
    "Al visualizar los datos se puede observar que las caracteristicas tienen diferentes magnitudes, por lo cual se debe transformar cada valor en una escala de valores similares, esto con el fin de que el descenso por el gradiente pueda converger mas rapidamente. En este caso\n",
    "\n",
    "Hacemos el uso de la siguiente funcion para normalizar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  featureNormalize(X):\n",
    "    X_norm = X.copy()\n",
    "\n",
    "    #creamos un array de ceros con una longitud igual al número de columnas en el array X. La variable mu y sigma se inicializa como este array de ceros.\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    #Creamos el promedio de cada filaa de X\n",
    "    #media de cada columna\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    \n",
    "    #desviacion estandar de cada fila de X\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    \n",
    "    sigma[sigma == 0] = 1\n",
    "    \n",
    "    #normalizamos los datos con la siguiente formula\n",
    "    X_norm = (X - mu) / sigma\n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion para el calculo de la sigmoide\n",
    "\n",
    "También conocida como la función logística, es una función matemática que toma cualquier número real como entrada y devuelve un valor en el rango de 0 a 1. Donde nuestra **Z** es nuestra hipotesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Calcula la sigmoide de una entrada z\n",
    "    # convierte la intrada a un arreglo numpy\n",
    "    z = np.array(z)\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de calculo de costo con regularizacion\n",
    "\n",
    "Para la regularizacion se implemento la suma de `+(lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))` a nuestra ecuacion de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularCostoCR(theta, X, y, lambda_):\n",
    "    # Inicializar algunos valores utiles\n",
    "    m = y.size  # numero de ejemplos de entrenamiento\n",
    "\n",
    "    J = 0\n",
    "    \n",
    "    # temp = theta.copy()\n",
    "    # temp[0] = 0\n",
    "    \n",
    "    #hacemos el uso de la funcion sigmoid\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
    "\n",
    "    # Calculamos el término de regularización (sin incluir el primer término de theta)\n",
    "    regularization_term = (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))\n",
    "\n",
    "    # Sumamos el término de regularización al costo total\n",
    "    J += regularization_term\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion de descenso por el gradiente con regularizacion\n",
    "\n",
    "\n",
    "Tambien introducimos el parametro lambda_ para aplicar regularizacion.\n",
    "\n",
    "Para el calculo de nuestras thetas sera\n",
    "\n",
    "para Theta 0 sera `theta[0] -= alpha * gradient[0]`.\n",
    "\n",
    "Para las demas thetas sera `theta[1:] -= alpha * (gradient[1:]+ regularization_term)`\n",
    "\n",
    "donde nuestra gradiente es `gradient = (1 / m) * X.T.dot(h - y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descensoGradienteCR(theta, X, y, alpha, lambda_, num_iters):\n",
    "    # Inicializa algunos valores\n",
    "    m = y.shape[0] # numero de ejemplos de entrenamiento\n",
    "\n",
    "    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente\n",
    "    theta = theta.copy()\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        h = sigmoid(X.dot(theta.T))\n",
    "        \n",
    "        # Calcula el gradiente descendente sin regularización\n",
    "        gradient = (1 / m) * X.T.dot(h - y)\n",
    "        \n",
    "        # Calcula el término de regularización (excepto para el término de sesgo theta[0])\n",
    "        regularization_term = (lambda_ / m) * theta[1:]\n",
    "        \n",
    "        # theta[0] -= alpha * (1 / m) * np.sum(h - y)\n",
    "        theta[0] -= alpha * gradient[0]\n",
    "        theta[1:] -= alpha * (gradient[1:]+ regularization_term)\n",
    "\n",
    "        # Calcula y guarda el costo en cada iteración\n",
    "        J_history.append(calcularCostoCR(theta, X, y, lambda_))\n",
    "        \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de calculo de costo sin regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularCostoSR(theta, X, y):\n",
    "    # Inicializar algunos valores utiles\n",
    "    m = y.size  # numero de ejemplos de entrenamiento\n",
    "\n",
    "    J = 0\n",
    "    \n",
    "    #hacemos el uso de la funcion sigmoid\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion de descenso por el gradiente sin regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descensoGradienteSR(theta, X, y, alpha, num_iters):\n",
    "    # Inicializa algunos valores\n",
    "    m = y.shape[0] # numero de ejemplos de entrenamiento\n",
    "\n",
    "    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente\n",
    "    theta = theta.copy()\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        h = sigmoid(X.dot(theta.T))\n",
    "        theta = theta - (alpha / m) * (h - y).dot(X)\n",
    "\n",
    "        J_history.append(calcularCostoSR(theta, X, y))\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Usando regularizacion\n",
    "\n",
    "La regularización es una técnica utilizada en el aprendizaje automático para prevenir el **sobreajuste (overfitting)** de un modelo a los datos de entrenamiento.\n",
    "\n",
    "El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento y captura el ruido o las fluctuaciones aleatorias en los datos en lugar de aprender la verdadera relación subyacente entre las características y la variable objetivo. Esto puede resultar en un rendimiento deficiente del modelo cuando se enfrenta a nuevos datos que no formaban parte del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Regresion Logistica\n",
    "\n",
    "Carga de los datos para la regresion logistica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]         Y\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "      50       1       173        72     120      70       1       1       0         0       1           1\n",
      "      62       2       174        52     120      80       1       1       1         0       1           1\n",
      "      46       2       156        62     115      70       1       1       0         0       1           0\n",
      "      58       1       158        90     140      90       1       1       0         0       1           1\n",
      "      62       1       156        90     160      80       3       3       0         0       1           0\n",
      "      60       1       151        44     120      80       1       2       0         0       1           0\n",
      "      40       1       156        56     120      70       1       1       0         0       1           0\n",
      "      45       1       157        43     120      80       1       1       0         0       1           0\n",
      "      56       1       167        78     130      80       3       3       0         0       1           1\n",
      "      40       2       174        82     120      80       1       1       1         0       1           0\n",
      " \n",
      "El 80% de ejemplos para entrenamiento son la cantidad de: 55026 de ejemplos\n",
      "El 20% de ejemplos para pruebas son la cantidad de: 13757 de ejemplos\n",
      "La cantidad total de ejemplos es de: 68783 de ejemplos\n"
     ]
    }
   ],
   "source": [
    "#hacemos una copia de y_train y y_test para usarlo en la regresion lineal multivariable\n",
    "\n",
    "#estos datos seran usados para el entrenamiento\n",
    "X_testCR = X_test.copy()\n",
    "y_testCR = y_test.copy()\n",
    "m_test_CR = len(y_testCR)\n",
    "\n",
    "#estos datos seran usados para el test\n",
    "X_trainCR = X_train.copy()\n",
    "y_trainCR = y_train.copy()\n",
    "m_train_CR = len(y_trainCR)\n",
    "\n",
    "#Imprimimos algunos datos:\n",
    "# imprimir todos las X de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'Y'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}{:10.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:10.0f}{:8.0f}{:12.0f}'.format(\n",
    "        X_trainCR[i, 0], \n",
    "        X_trainCR[i, 1],\n",
    "        X_trainCR[i, 2],\n",
    "        X_trainCR[i, 3], \n",
    "        X_trainCR[i, 4], \n",
    "        X_trainCR[i, 5],\n",
    "        X_trainCR[i, 6], \n",
    "        X_trainCR[i, 7], \n",
    "        X_trainCR[i, 8], \n",
    "        X_trainCR[i, 9], \n",
    "        X_trainCR[i, 10], \n",
    "        y_trainCR[i]\n",
    "    ))\n",
    "\n",
    "#mostramos la cantidad de ejemplos\n",
    "print(\" \")\n",
    "print('El 80% de ejemplos para entrenamiento son la cantidad de: {:.0f} de ejemplos'.format( len(train_dataset)))\n",
    "print('El 20% de ejemplos para pruebas son la cantidad de: {:.0f} de ejemplos'.format( len(test_dataset)))\n",
    "print('La cantidad total de ejemplos es de: {:.0f} de ejemplos'.format( len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Normalizacion de las caracteristicas\n",
    "\n",
    "Se hace uso de la funcion de `featureNormalize(X) ` donde se recibe un parametro de tipo matriz para normalizar cada dato dentro de ella, retornandome la **matriz normalizda**, **sigma(desviacion estandar)**, y mi **mu(media)**.\n",
    "\n",
    "Almacenando los datos normalizados en **X_norm** usando la funcion **featureNormaliza()**, normalizando los datos de X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  -0.495  -0.730     1.058    -0.146  -0.393  -1.183  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   1.281   1.370     1.180    -1.543  -0.393  -0.143  -0.538  -0.396   3.226    -0.238   0.497\n",
      "  -1.087   1.370    -1.024    -0.844  -0.692  -1.183  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   0.689  -0.730    -0.779     1.112   0.803   0.896  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   1.281  -0.730    -1.024     1.112   1.999  -0.143   2.401   3.085  -0.310    -0.238   0.497\n",
      "   0.985  -0.730    -1.636    -2.102  -0.393  -0.143  -0.538   1.345  -0.310    -0.238   0.497\n",
      "  -1.974  -0.730    -1.024    -1.263  -0.393  -1.183  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "  -1.235  -0.730    -0.901    -2.172  -0.393  -0.143  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   0.393  -0.730     0.323     0.274   0.205  -0.143   2.401   3.085  -0.310    -0.238   0.497\n",
      "  -1.974   1.370     1.180     0.553  -0.393  -0.143  -0.538  -0.396   3.226    -0.238   0.497\n"
     ]
    }
   ],
   "source": [
    "X_norm_CR, mu_CR, sigma_CR= featureNormalize(X_trainCR)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}'.format(\n",
    "        X_norm_CR[i, 0], \n",
    "        X_norm_CR[i, 1],\n",
    "        X_norm_CR[i, 2], \n",
    "        X_norm_CR[i, 3], \n",
    "        X_norm_CR[i, 4], \n",
    "        X_norm_CR[i, 5],\n",
    "        X_norm_CR[i, 6],\n",
    "        X_norm_CR[i, 7], \n",
    "        X_norm_CR[i, 8], \n",
    "        X_norm_CR[i, 9], \n",
    "        X_norm_CR[i, 10]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Agregamos la columna de unos a nuestra matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "   1.000  -0.495    -0.730     1.058  -0.146  -0.393  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   1.281     1.370     1.180  -1.543  -0.393  -0.143  -0.538  -0.396     3.226  -0.238   0.497\n",
      "   1.000  -1.087     1.370    -1.024  -0.844  -0.692  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.689    -0.730    -0.779   1.112   0.803   0.896  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   1.281    -0.730    -1.024   1.112   1.999  -0.143   2.401   3.085    -0.310  -0.238   0.497\n",
      "   1.000   0.985    -0.730    -1.636  -2.102  -0.393  -0.143  -0.538   1.345    -0.310  -0.238   0.497\n",
      "   1.000  -1.974    -0.730    -1.024  -1.263  -0.393  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -1.235    -0.730    -0.901  -2.172  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.393    -0.730     0.323   0.274   0.205  -0.143   2.401   3.085    -0.310  -0.238   0.497\n",
      "   1.000  -1.974     1.370     1.180   0.553  -0.393  -0.143  -0.538  -0.396     3.226  -0.238   0.497\n"
     ]
    }
   ],
   "source": [
    "X_ready_CR = np.concatenate([np.ones((m_train_CR, 1)), X_norm_CR], axis=1)\n",
    "\n",
    "# print(len(X_ready[0]))\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]'\n",
    "))\n",
    "print('-' * 130)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}{:8.3f}'.format(\n",
    "        X_ready_CR[i, 0],\n",
    "        X_ready_CR[i, 1], \n",
    "        X_ready_CR[i, 2], \n",
    "        X_ready_CR[i, 3],\n",
    "        X_ready_CR[i, 4], \n",
    "        X_ready_CR[i, 5], \n",
    "        X_ready_CR[i, 6], \n",
    "        X_ready_CR[i, 7], \n",
    "        X_ready_CR[i, 8], \n",
    "        X_ready_CR[i, 9], \n",
    "        X_ready_CR[i, 10], \n",
    "        X_ready_CR[i, 11]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Descenso por el gradiente\n",
    "\n",
    "Al igual que regresion lineal se aplicara el descenso por la gradiente, con la diferencia que aqui se hara el uso de la funcion **sigmoid()**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.1 Cálculo del costo $J(\\theta)$\n",
    "\n",
    "hacemos uso de la funcion def `calcularCosto(X, y, theta, lambda_)`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de funcionamiento de la funcion computeCoste con dos valores diferentes de $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "########################################################################################################\n",
      "con theta:[0.2 3.  0.2 0.1 3.  0.2 1.1 2.  0.7 0.8 5.8 0.9] se obtiene un costo de: nan\n",
      "########################################################################################################\n",
      "con theta:[0.1 2.  0.5 0.2 8.  0.7 1.3 8.  1.7 0.7 7.1 7.2] se obtiene un costo de: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANIMETX\\AppData\\Local\\Temp\\ipykernel_12464\\294937797.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n"
     ]
    }
   ],
   "source": [
    "theta_CR=np.array([0.2, 3.0, 0.2,0.1, 3.0, 0.2,1.1, 2.0, 0.7,0.8, 5.8, 0.9])\n",
    "theta_CR1=np.array([0.1, 2.0, 0.5,0.2, 8.0, 0.7,1.3, 8.0, 1.7,0.7, 7.1, 7.2])\n",
    "\n",
    "lambda_ = 1000\n",
    "print(theta_CR.shape[0])\n",
    "print(f\"########################################################################################################\")\n",
    "print(f\"con theta:{ theta_CR } se obtiene un costo de: {calcularCostoCR(theta_CR, X_ready_CR, y_trainCR, lambda_)}\")\n",
    "print(f\"########################################################################################################\")\n",
    "print(f\"con theta:{ theta_CR1 } se obtiene un costo de: {calcularCostoCR(theta_CR ,X_ready_CR, y_trainCR, lambda_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2Descenso por el gradiente\n",
    "\n",
    "Hacemos uso de la funcion para hacer el calculo del descenso por el gradiente y asi encontrar nuestras **Thetas**, se hizo la modificacion para que la funcion ahora reciba el parametro de ``lamda_``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se inicializan los parametros $\\theta$ con 0 y la taza de aprendizaje $\\alpha$ con 0.00009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################\n",
      "Los valores de theta calculados son:\n",
      "theta 1: 0.0010606206096008743\n",
      "theta 2: 0.3123301913325096\n",
      "theta 3: -0.013163592778050879\n",
      "theta 4: -0.031167871213402865\n",
      "theta 5: 0.1613032296745273\n",
      "theta 6: 0.6248055907344151\n",
      "theta 7: 0.29086541051901227\n",
      "theta 8: 0.2793483318857407\n",
      "theta 9: -0.01323984185260168\n",
      "theta 10: -0.03752908286722199\n",
      "theta 11: -0.03814165935433397\n",
      "theta 12: -0.08010965261927805\n",
      "########################################################################################################\n",
      "con un costo de: 0.572162951901662 \n"
     ]
    }
   ],
   "source": [
    "#creamos un theta con 19 columnas de ceros\n",
    "theta_CR = np.zeros(len(X_ready_CR[0]))\n",
    "\n",
    "#numero de iteraciones sera 900 y un alpha 0.009\n",
    "num_ite_CR = 15000\n",
    "alpha_CR = 0.0009\n",
    "lambda_CR = 1000\n",
    "\n",
    "theta_CR, J_historico_CR = descensoGradienteCR(theta_CR, X_ready_CR, y_trainCR, alpha_CR, lambda_CR, num_ite_CR)\n",
    "\n",
    "print(\"########################################################################################################\")\n",
    "print(\"Los valores de theta calculados son:\")\n",
    "i = 0\n",
    "for tht in theta_CR:\n",
    "    i += 1\n",
    "    print(f\"theta {i}: {tht}\")\n",
    "\n",
    "\n",
    "print(f\"########################################################################################################\")\n",
    "#mostramos el ultimo costo, este seria el mejor costo\n",
    "print(f\"con un costo de: { J_historico_CR[-1]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Grafica de la convergencia del costo\n",
    "graficamos el costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Costo J')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(np.arange(len(J_historico_CR)), J_historico_CR, lw=2)\n",
    "pyplot.title(\"Grafica de la convergencia del costo\")\n",
    "pyplot.xlabel('Numero de iteraciones')\n",
    "pyplot.ylabel('Costo J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo la prueba con un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Una persona con las caracteristicas: [1, 58, 1, 159, 53, 110, 70, 1, 1, 0, 0, 1] \n",
      "Tiene una probabilidad de tener diabetes de: 24.62338817330889 %\n",
      "Con valores de theta: [ 0.00106062  0.31233019 -0.01316359 -0.03116787  0.16130323  0.62480559\n",
      "  0.29086541  0.27934833 -0.01323984 -0.03752908 -0.03814166 -0.08010965]\n"
     ]
    }
   ],
   "source": [
    "X_array_CR = [1,58,1,159,53,110,70,1,1,0,0,1]\n",
    "X_array_copy_CR = X_array_CR.copy()\n",
    "#Se normaliza las caracteristicas para la prueba. haciendo el uso de mu y sigma calculados anteriormente, solamente los valores despues del primero, porque este es el 1.\n",
    "X_array_CR[1:] = (X_array_CR[1:] - mu_CR) / sigma_CR\n",
    "\n",
    "resultados_CR = sigmoid(np.dot(X_array_CR, theta_CR)) \n",
    "\n",
    "print(f\"Una persona con las caracteristicas: {X_array_copy_CR} \")\n",
    "print(f'Tiene una probabilidad de tener diabetes de: {resultados_CR * 100} %')\n",
    "\n",
    "print(f\"Con valores de theta: { theta_CR }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 Ejemplos de Predicciones\n",
    "\n",
    "Se creo una matriz con 11 ejemplos, donde se hace las predicciones correspondientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.5.1 Definiendo nuestro umbral clasificador\n",
    "Donde:\n",
    "\n",
    "* Si $h(\\theta)$ >= 0.5, predice \"y = 1\".\n",
    "* Si $h(\\theta)$ < 0.5 , predice \"y = 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|   AGE |   GENDER |   HEIGHT |   WEIGHT |   AP_HIGH |   AP_LOW |   CHOLESTEROL |   GLUCOSE |   SMOKE |   ALCOHOL |   PHYSICAL_ACTIVITY |   CARDIO_DISEASE |   CARDIO_DISEASE(Si/No) |\n",
      "+=======+==========+==========+==========+===========+==========+===============+===========+=========+===========+=====================+==================+=========================+\n",
      "|    50 |        2 |      168 |       62 |       110 |       80 |             1 |         1 |       0 |         0 |                   1 |         0.241085 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    62 |        1 |      165 |       68 |       150 |       80 |             2 |         1 |       0 |         0 |                   0 |         0.834864 |                       1 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    58 |        1 |      153 |       78 |       140 |       90 |             2 |         1 |       0 |         0 |                   1 |         0.789469 |                       1 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    46 |        1 |      169 |       64 |       120 |       80 |             3 |         1 |       0 |         0 |                   1 |         0.477367 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    46 |        1 |      158 |       58 |       110 |       80 |             1 |         1 |       0 |         0 |                   1 |         0.212344 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    60 |        2 |      170 |       69 |       120 |       80 |             1 |         1 |       1 |         1 |                   1 |         0.367801 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    52 |        2 |      171 |       98 |       110 |       90 |             1 |         1 |       0 |         0 |                   1 |         0.411493 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    52 |        1 |      153 |       63 |       110 |       70 |             2 |         1 |       0 |         0 |                   1 |         0.29944  |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    59 |        2 |      165 |       65 |       120 |       80 |             1 |         1 |       0 |         0 |                   1 |         0.422627 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    64 |        1 |      148 |       50 |       120 |       80 |             2 |         1 |       0 |         0 |                   1 |         0.562922 |                       1 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    54 |        2 |      169 |       55 |       120 |       80 |             1 |         1 |       1 |         0 |                   1 |         0.309236 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "nombres_columnas = ['AGE','GENDER','HEIGHT','WEIGHT','AP_HIGH','AP_LOW','CHOLESTEROL','GLUCOSE','SMOKE','ALCOHOL','PHYSICAL_ACTIVITY','CARDIO_DISEASE', 'CARDIO_DISEASE(Si/No)']\n",
    "\n",
    "matriz_datos_CR = np.array([\n",
    "[50,2,168,62,110,80,1,1,0,0,1],\n",
    "[62,1,165,68,150,80,2,1,0,0,0],\n",
    "[58,1,153,78,140,90,2,1,0,0,1],\n",
    "[46,1,169,64,120,80,3,1,0,0,1],\n",
    "[46,1,158,58,110,80,1,1,0,0,1],\n",
    "[60,2,170,69,120,80,1,1,1,1,1],\n",
    "[52,2,171,98,110,90,1,1,0,0,1],\n",
    "[52,1,153,63,110,70,2,1,0,0,1],\n",
    "[59,2,165,65,120,80,1,1,0,0,1],\n",
    "[64,1,148,50,120,80,2,1,0,0,1],\n",
    "[54,2,169,55,120,80,1,1,1,0,1],\n",
    "])\n",
    "\n",
    "para_tabla = matriz_datos_CR.copy()\n",
    "#creamos un vector parta almacenar cada Y predicha\n",
    "y_pre_CR = []\n",
    "\n",
    "matriz_datos_CR = (matriz_datos_CR- mu_CR) / sigma_CR\n",
    "matriz_datos_CR = np.concatenate([np.ones((len(matriz_datos_CR), 1)), matriz_datos_CR], axis=1)\n",
    "\n",
    "# Calculamos la Y predicha de los 11 ejemplos de prediccion\n",
    "# Calculamos la Y predicha de cada fila de la matriz\n",
    "for j in matriz_datos_CR:\n",
    "    y_pre_CR.append(sigmoid(np.dot(j, theta_CR)))\n",
    "\n",
    "# Convertimos la lista a un array unidimensional\n",
    "\n",
    "y_pre_CR = np.array(y_pre_CR)\n",
    "\n",
    "# usamos umbral para definir si tiene o no la enfermedad\n",
    "y_pre_umbral_CR = (y_pre_CR >= 0.5).astype(int)\n",
    "\n",
    "para_tabla = np.column_stack((para_tabla, y_pre_CR))\n",
    "para_tabla = np.column_stack((para_tabla, y_pre_umbral_CR))\n",
    "# Convertir la matriz en una lista de listas\n",
    "datos_para_tabla = para_tabla.tolist()\n",
    "\n",
    "# Imprimir la tabla\n",
    "print(tabulate(datos_para_tabla, headers=nombres_columnas, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6 Validaciones\n",
    "Para hacer las validaciones correspondientes se hizo el uso siguiendo el consejo de 80/20, donde 80% es para la fase de entrenamiento, y 20% es para la fase de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.6.1 Normalizamos el X_test que es el 20% separado a un incio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_test_CR = (X_testCR- mu_CR) / sigma_CR\n",
    "m_test= len(X_testCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.6.2 Concadenamos unos a matriz X normalizado del test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "   1.000  -0.643    -0.730    -1.024  -0.844  -1.589  -1.183   0.931  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.393    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.541    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.097    -0.730     0.323   2.160   1.999   1.935  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.199    -0.730    -2.982   0.204   0.803   0.896  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.495    -0.730     0.445  -0.076   1.401   0.896  -0.538  -0.396    -0.310  -0.238  -2.014\n",
      "   1.000   1.281     1.370    -0.534   0.902  -0.991  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.643    -0.730    -1.146   1.671  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.199     1.370     0.078  -0.285   0.205  -0.143  -0.538  -0.396     3.226  -0.238   0.497\n",
      "   1.000   0.393     1.370     0.568  -0.425   0.803   0.896   0.931  -0.396    -0.310  -0.238   0.497\n"
     ]
    }
   ],
   "source": [
    "X_test_ready_CR = np.concatenate([np.ones((m_test_CR, 1)), X_norm_test_CR], axis=1)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}{:8.3f}'.format(\n",
    "    X_test_ready_CR[i, 0],\n",
    "    X_test_ready_CR[i, 1],\n",
    "    X_test_ready_CR[i, 2],\n",
    "    X_test_ready_CR[i, 3], \n",
    "    X_test_ready_CR[i, 4],\n",
    "    X_test_ready_CR[i, 5], \n",
    "    X_test_ready_CR[i, 6],\n",
    "    X_test_ready_CR[i, 7], \n",
    "    X_test_ready_CR[i, 8], \n",
    "    X_test_ready_CR[i, 9], \n",
    "    X_test_ready_CR[i, 10], \n",
    "    X_test_ready_CR[i, 11]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.6.3 Hacemos el calculo de Y predicha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]         y     (y) usando el umbral\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   1.000  -0.643    -0.730    -1.024  -0.844  -1.589  -1.183   0.931  -0.396    -0.310  -0.238   0.497           0.20              0\n",
      "   1.000   0.393    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497           0.43              0\n",
      "   1.000   0.541    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497           0.44              0\n",
      "   1.000   0.097    -0.730     0.323   2.160   1.999   1.935  -0.538  -0.396    -0.310  -0.238   0.497           0.88              1\n",
      "   1.000  -0.199    -0.730    -2.982   0.204   0.803   0.896  -0.538  -0.396    -0.310  -0.238   0.497           0.66              1\n",
      "   1.000  -0.495    -0.730     0.445  -0.076   1.401   0.896  -0.538  -0.396    -0.310  -0.238  -2.014           0.73              1\n",
      "   1.000   1.281     1.370    -0.534   0.902  -0.991  -1.183  -0.538  -0.396    -0.310  -0.238   0.497           0.36              0\n",
      "   1.000  -0.643    -0.730    -1.146   1.671  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497           0.42              0\n",
      "   1.000  -0.199     1.370     0.078  -0.285   0.205  -0.143  -0.538  -0.396     3.226  -0.238   0.497           0.42              0\n",
      "   1.000   0.393     1.370     0.568  -0.425   0.803   0.896   0.931  -0.396    -0.310  -0.238   0.497           0.74              1\n"
     ]
    }
   ],
   "source": [
    "y_predicha_CR =[]\n",
    "# Calculamos la Y predicha de cada fila de la matriz\n",
    "for dato in X_test_ready_CR:\n",
    "    y_predicha_CR.append(sigmoid(np.dot(dato, theta_CR.T)))\n",
    "\n",
    "# Convertimos la lista a un array unidimensional\n",
    "y_predicha_CR = np.array(y_predicha_CR)\n",
    "\n",
    "#usando el umbral donde todo aquello que sea >= 0.5 sera 1, y si es menor sera 0\n",
    "y_umbral_CR = (y_predicha_CR >= 0.5).astype(int)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}{:>10s}{:>25s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]','y','(y) usando el umbral'\n",
    "))\n",
    "print('-' * 140)\n",
    "\n",
    "#Mostrando algunos datos\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}{:8.3f}{:15.2f}{:15.0f}'.format(\n",
    "    X_test_ready_CR[i, 0],\n",
    "    X_test_ready_CR[i, 1], \n",
    "    X_test_ready_CR[i, 2], \n",
    "    X_test_ready_CR[i, 3],\n",
    "    X_test_ready_CR[i, 4], \n",
    "    X_test_ready_CR[i, 5], \n",
    "    X_test_ready_CR[i, 6],\n",
    "    X_test_ready_CR[i, 7], \n",
    "    X_test_ready_CR[i, 8], \n",
    "    X_test_ready_CR[i, 9], \n",
    "    X_test_ready_CR[i, 10],\n",
    "    X_test_ready_CR[i, 11], \n",
    "    y_predicha_CR[i], \n",
    "    y_umbral_CR[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.6.4 Calculando la precision del entrenamiento:\n",
    "se hace uso del **np.mean**, Calcula la media (promedio) de los valores booleanos. Dado que True se interpreta como 1 y False como 0 en operaciones aritméticas, la media resultante será la proporción de elementos iguales en **y_predicha** e **y_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de entrenamiento: 72.89380 %\n"
     ]
    }
   ],
   "source": [
    "print('Precisión de entrenamiento: {:.5f} %'.format(np.mean(y_umbral_CR == y_testCR) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Usando sin regularizacion\n",
    "\n",
    "En este paso se aplicara los mismos pasos, solo que no se aplicara la regularizacion  para evitar el **sobreajuste (overfitting)** de un modelo a los datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Regresion Logistica\n",
    "\n",
    "Carga de los datos para la regresion logistica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]         Y\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "      50       1       173        72     120      70       1       1       0         0       1           1\n",
      "      62       2       174        52     120      80       1       1       1         0       1           1\n",
      "      46       2       156        62     115      70       1       1       0         0       1           0\n",
      "      58       1       158        90     140      90       1       1       0         0       1           1\n",
      "      62       1       156        90     160      80       3       3       0         0       1           0\n",
      "      60       1       151        44     120      80       1       2       0         0       1           0\n",
      "      40       1       156        56     120      70       1       1       0         0       1           0\n",
      "      45       1       157        43     120      80       1       1       0         0       1           0\n",
      "      56       1       167        78     130      80       3       3       0         0       1           1\n",
      "      40       2       174        82     120      80       1       1       1         0       1           0\n",
      " \n",
      "El 80% de ejemplos para entrenamiento son la cantidad de: 55026 de ejemplos\n",
      "El 20% de ejemplos para pruebas son la cantidad de: 13757 de ejemplos\n",
      "La cantidad total de ejemplos es de: 68783 de ejemplos\n"
     ]
    }
   ],
   "source": [
    "#hacemos una copia de y_train y y_test para usarlo en la regresion lineal multivariable\n",
    "\n",
    "#estos datos seran usados para el entrenamiento\n",
    "X_testSR = X_test.copy()\n",
    "y_testSR = y_test.copy()\n",
    "m_test_SR = len(y_testSR)\n",
    "\n",
    "#estos datos seran usados para el test\n",
    "X_trainSR = X_train.copy()\n",
    "y_trainSR = y_train.copy()\n",
    "m_train_SR = len(y_trainSR)\n",
    "\n",
    "#Imprimimos algunos datos:\n",
    "# imprimir todos las X de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'Y'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}{:10.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:10.0f}{:8.0f}{:12.0f}'.format(\n",
    "        X_trainSR[i, 0], \n",
    "        X_trainSR[i, 1],\n",
    "        X_trainSR[i, 2],\n",
    "        X_trainSR[i, 3], \n",
    "        X_trainSR[i, 4], \n",
    "        X_trainSR[i, 5],\n",
    "        X_trainSR[i, 6], \n",
    "        X_trainSR[i, 7], \n",
    "        X_trainSR[i, 8], \n",
    "        X_trainSR[i, 9], \n",
    "        X_trainSR[i, 10], \n",
    "        y_trainSR[i]\n",
    "    ))\n",
    "\n",
    "#mostramos la cantidad de ejemplos\n",
    "print(\" \")\n",
    "print('El 80% de ejemplos para entrenamiento son la cantidad de: {:.0f} de ejemplos'.format( len(train_dataset)))\n",
    "print('El 20% de ejemplos para pruebas son la cantidad de: {:.0f} de ejemplos'.format( len(test_dataset)))\n",
    "print('La cantidad total de ejemplos es de: {:.0f} de ejemplos'.format( len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Normalizacion de las caracteristicas\n",
    "\n",
    "Se hace uso de la funcion de `featureNormalize(X) ` para normalizar todos los valores de nuestra matriz X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "  -0.495  -0.730     1.058    -0.146  -0.393  -1.183  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   1.281   1.370     1.180    -1.543  -0.393  -0.143  -0.538  -0.396   3.226    -0.238   0.497\n",
      "  -1.087   1.370    -1.024    -0.844  -0.692  -1.183  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   0.689  -0.730    -0.779     1.112   0.803   0.896  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   1.281  -0.730    -1.024     1.112   1.999  -0.143   2.401   3.085  -0.310    -0.238   0.497\n",
      "   0.985  -0.730    -1.636    -2.102  -0.393  -0.143  -0.538   1.345  -0.310    -0.238   0.497\n",
      "  -1.974  -0.730    -1.024    -1.263  -0.393  -1.183  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "  -1.235  -0.730    -0.901    -2.172  -0.393  -0.143  -0.538  -0.396  -0.310    -0.238   0.497\n",
      "   0.393  -0.730     0.323     0.274   0.205  -0.143   2.401   3.085  -0.310    -0.238   0.497\n",
      "  -1.974   1.370     1.180     0.553  -0.393  -0.143  -0.538  -0.396   3.226    -0.238   0.497\n"
     ]
    }
   ],
   "source": [
    "X_norm_SR, mu_SR, sigma_SR= featureNormalize(X_trainSR)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}'.format(\n",
    "        X_norm_SR[i, 0], \n",
    "        X_norm_SR[i, 1],\n",
    "        X_norm_SR[i, 2], \n",
    "        X_norm_SR[i, 3], \n",
    "        X_norm_SR[i, 4], \n",
    "        X_norm_SR[i, 5],\n",
    "        X_norm_SR[i, 6],\n",
    "        X_norm_SR[i, 7], \n",
    "        X_norm_SR[i, 8], \n",
    "        X_norm_SR[i, 9], \n",
    "        X_norm_SR[i, 10]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Agregamos la columna de unos a nuestra matriz\n",
    "\n",
    "Agregamos nuestro sesgo de columna de unos a nuestra matriz de X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "   1.000  -0.495    -0.730     1.058  -0.146  -0.393  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   1.281     1.370     1.180  -1.543  -0.393  -0.143  -0.538  -0.396     3.226  -0.238   0.497\n",
      "   1.000  -1.087     1.370    -1.024  -0.844  -0.692  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.689    -0.730    -0.779   1.112   0.803   0.896  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   1.281    -0.730    -1.024   1.112   1.999  -0.143   2.401   3.085    -0.310  -0.238   0.497\n",
      "   1.000   0.985    -0.730    -1.636  -2.102  -0.393  -0.143  -0.538   1.345    -0.310  -0.238   0.497\n",
      "   1.000  -1.974    -0.730    -1.024  -1.263  -0.393  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -1.235    -0.730    -0.901  -2.172  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.393    -0.730     0.323   0.274   0.205  -0.143   2.401   3.085    -0.310  -0.238   0.497\n",
      "   1.000  -1.974     1.370     1.180   0.553  -0.393  -0.143  -0.538  -0.396     3.226  -0.238   0.497\n"
     ]
    }
   ],
   "source": [
    "X_ready_SR = np.concatenate([np.ones((m_train_SR, 1)), X_norm_SR], axis=1)\n",
    "\n",
    "# print(len(X_ready[0]))\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]'\n",
    "))\n",
    "print('-' * 130)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}{:8.3f}'.format(\n",
    "        X_ready_SR[i, 0],\n",
    "        X_ready_SR[i, 1], \n",
    "        X_ready_SR[i, 2], \n",
    "        X_ready_SR[i, 3],\n",
    "        X_ready_SR[i, 4], \n",
    "        X_ready_SR[i, 5], \n",
    "        X_ready_SR[i, 6], \n",
    "        X_ready_SR[i, 7], \n",
    "        X_ready_SR[i, 8], \n",
    "        X_ready_SR[i, 9], \n",
    "        X_ready_SR[i, 10], \n",
    "        X_ready_SR[i, 11]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3.2Descenso por el gradiente\n",
    "\n",
    "Hacemos uso de la funcion para hacer el calculo del descenso por el gradiente pero esta ves sin introducir el parametro `lambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################\n",
      "Los valores de theta calculados son:\n",
      "theta 1: 0.0041007900367013616\n",
      "theta 2: 0.33329366842114116\n",
      "theta 3: -0.01440155045181225\n",
      "theta 4: -0.03335108664096205\n",
      "theta 5: 0.16883752264354393\n",
      "theta 6: 0.6747839473121756\n",
      "theta 7: 0.3028748861970188\n",
      "theta 8: 0.2986999295322526\n",
      "theta 9: -0.02134335165248756\n",
      "theta 10: -0.040318407936396045\n",
      "theta 11: -0.04193558900750641\n",
      "theta 12: -0.08690754869341903\n",
      "########################################################################################################\n",
      "con un costo de: 0.5647883697613265 \n"
     ]
    }
   ],
   "source": [
    "#creamos un theta con 19 columnas de ceros\n",
    "theta_SR = np.zeros(len(X_ready_SR[0]))\n",
    "\n",
    "#numero de iteraciones sera 900 y un alpha 0.009\n",
    "num_ite_SR = 15000\n",
    "alpha_SR = 0.0009\n",
    "\n",
    "theta_SR, J_historico_SR = descensoGradienteSR(theta_SR, X_ready_SR, y_trainSR, alpha_SR, num_ite_SR)\n",
    "\n",
    "print(\"########################################################################################################\")\n",
    "print(\"Los valores de theta calculados son:\")\n",
    "i = 0\n",
    "for tht_SR in theta_SR:\n",
    "    i += 1\n",
    "    print(f\"theta {i}: {tht_SR}\")\n",
    "\n",
    "\n",
    "print(f\"########################################################################################################\")\n",
    "#mostramos el ultimo costo, este seria el mejor costo\n",
    "print(f\"con un costo de: { J_historico_SR[-1]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Grafica de la convergencia del costo\n",
    "graficamos el costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Costo J')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(np.arange(len(J_historico_SR)), J_historico_SR, lw=2)\n",
    "pyplot.title(\"Grafica de la convergencia del costo\")\n",
    "pyplot.xlabel('Numero de iteraciones')\n",
    "pyplot.ylabel('Costo J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo la prueba con un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Una persona con las caracteristicas: [1, 58, 1, 159, 53, 110, 70, 1, 1, 0, 0, 1] \n",
      "Tiene una probabilidad de tener diabetes de: 23.457839572489682 %\n",
      "Con valores de theta: [ 0.00410079  0.33329367 -0.01440155 -0.03335109  0.16883752  0.67478395\n",
      "  0.30287489  0.29869993 -0.02134335 -0.04031841 -0.04193559 -0.08690755]\n"
     ]
    }
   ],
   "source": [
    "X_array_SR = [1,58,1,159,53,110,70,1,1,0,0,1]\n",
    "X_array_copy_SR = X_array_SR.copy()\n",
    "#Se normaliza las caracteristicas para la prueba. haciendo el uso de mu y sigma calculados anteriormente, solamente los valores despues del primero, porque este es el 1.\n",
    "X_array_SR[1:] = (X_array_SR[1:] - mu_SR) / sigma_SR\n",
    "\n",
    "resultados_SR = sigmoid(np.dot(X_array_SR, theta_SR)) \n",
    "\n",
    "print(f\"Una persona con las caracteristicas: {X_array_copy_SR} \")\n",
    "print(f'Tiene una probabilidad de tener diabetes de: {resultados_SR * 100} %')\n",
    "\n",
    "print(f\"Con valores de theta: { theta_SR }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.5.1 Definiendo nuestro umbral clasificador\n",
    "Donde:\n",
    "\n",
    "* Si $h(\\theta)$ >= 0.5, predice \"y = 1\".\n",
    "* Si $h(\\theta)$ < 0.5 , predice \"y = 0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|   AGE |   GENDER |   HEIGHT |   WEIGHT |   AP_HIGH |   AP_LOW |   CHOLESTEROL |   GLUCOSE |   SMOKE |   ALCOHOL |   PHYSICAL_ACTIVITY |   CARDIO_DISEASE |   CARDIO_DISEASE(Si/No) |\n",
      "+=======+==========+==========+==========+===========+==========+===============+===========+=========+===========+=====================+==================+=========================+\n",
      "|    50 |        2 |      168 |       62 |       110 |       80 |             1 |         1 |       0 |         0 |                   1 |         0.227378 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    62 |        1 |      165 |       68 |       150 |       80 |             2 |         1 |       0 |         0 |                   0 |         0.852316 |                       1 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    58 |        1 |      153 |       78 |       140 |       90 |             2 |         1 |       0 |         0 |                   1 |         0.804665 |                       1 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    46 |        1 |      169 |       64 |       120 |       80 |             3 |         1 |       0 |         0 |                   1 |         0.477693 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    46 |        1 |      158 |       58 |       110 |       80 |             1 |         1 |       0 |         0 |                   1 |         0.198368 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    60 |        2 |      170 |       69 |       120 |       80 |             1 |         1 |       1 |         1 |                   1 |         0.358762 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    52 |        2 |      171 |       98 |       110 |       90 |             1 |         1 |       0 |         0 |                   1 |         0.401937 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    52 |        1 |      153 |       63 |       110 |       70 |             2 |         1 |       0 |         0 |                   1 |         0.289645 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    59 |        2 |      165 |       65 |       120 |       80 |             1 |         1 |       0 |         0 |                   1 |         0.418665 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    64 |        1 |      148 |       50 |       120 |       80 |             2 |         1 |       0 |         0 |                   1 |         0.569538 |                       1 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n",
      "|    54 |        2 |      169 |       55 |       120 |       80 |             1 |         1 |       1 |         0 |                   1 |         0.299086 |                       0 |\n",
      "+-------+----------+----------+----------+-----------+----------+---------------+-----------+---------+-----------+---------------------+------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "nombres_columnas = ['AGE','GENDER','HEIGHT','WEIGHT','AP_HIGH','AP_LOW','CHOLESTEROL','GLUCOSE','SMOKE','ALCOHOL','PHYSICAL_ACTIVITY','CARDIO_DISEASE', 'CARDIO_DISEASE(Si/No)']\n",
    "\n",
    "matriz_datos_SR = np.array([\n",
    "[50,2,168,62,110,80,1,1,0,0,1],\n",
    "[62,1,165,68,150,80,2,1,0,0,0],\n",
    "[58,1,153,78,140,90,2,1,0,0,1],\n",
    "[46,1,169,64,120,80,3,1,0,0,1],\n",
    "[46,1,158,58,110,80,1,1,0,0,1],\n",
    "[60,2,170,69,120,80,1,1,1,1,1],\n",
    "[52,2,171,98,110,90,1,1,0,0,1],\n",
    "[52,1,153,63,110,70,2,1,0,0,1],\n",
    "[59,2,165,65,120,80,1,1,0,0,1],\n",
    "[64,1,148,50,120,80,2,1,0,0,1],\n",
    "[54,2,169,55,120,80,1,1,1,0,1],\n",
    "])\n",
    "\n",
    "para_tabla = matriz_datos_SR.copy()\n",
    "#creamos un vector parta almacenar cada Y predicha\n",
    "y_pre_SR = []\n",
    "\n",
    "matriz_datos_SR = (matriz_datos_SR- mu_SR) / sigma_SR\n",
    "matriz_datos_SR = np.concatenate([np.ones((len(matriz_datos_SR), 1)), matriz_datos_SR], axis=1)\n",
    "\n",
    "# Calculamos la Y predicha de los 11 ejemplos de prediccion\n",
    "# Calculamos la Y predicha de cada fila de la matriz\n",
    "for j in matriz_datos_SR:\n",
    "    y_pre_SR.append(sigmoid(np.dot(j, theta_SR)))\n",
    "\n",
    "# Convertimos la lista a un array unidimensional\n",
    "\n",
    "y_pre_SR = np.array(y_pre_SR)\n",
    "\n",
    "# usamos umbral para definir si tiene o no la enfermedad\n",
    "y_pre_umbral_SR = (y_pre_SR >= 0.5).astype(int)\n",
    "\n",
    "para_tabla = np.column_stack((para_tabla, y_pre_SR))\n",
    "para_tabla = np.column_stack((para_tabla, y_pre_umbral_SR))\n",
    "# Convertir la matriz en una lista de listas\n",
    "datos_para_tabla = para_tabla.tolist()\n",
    "\n",
    "# Imprimir la tabla\n",
    "print(tabulate(datos_para_tabla, headers=nombres_columnas, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 Validaciones\n",
    "Para hacer las validaciones correspondientes se hizo el uso siguiendo el consejo de 80/20, donde 80% es para la fase de entrenamiento, y 20% es para la fase de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.6.1 Normalizamos el X_test que es el 20% separado a un incio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_test_SR = (X_testSR- mu_SR) / sigma_SR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.6.2 Concadenamos unos a matriz X normalizado del test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "   1.000  -0.643    -0.730    -1.024  -0.844  -1.589  -1.183   0.931  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.393    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.541    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000   0.097    -0.730     0.323   2.160   1.999   1.935  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.199    -0.730    -2.982   0.204   0.803   0.896  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.495    -0.730     0.445  -0.076   1.401   0.896  -0.538  -0.396    -0.310  -0.238  -2.014\n",
      "   1.000   1.281     1.370    -0.534   0.902  -0.991  -1.183  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.643    -0.730    -1.146   1.671  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497\n",
      "   1.000  -0.199     1.370     0.078  -0.285   0.205  -0.143  -0.538  -0.396     3.226  -0.238   0.497\n",
      "   1.000   0.393     1.370     0.568  -0.425   0.803   0.896   0.931  -0.396    -0.310  -0.238   0.497\n"
     ]
    }
   ],
   "source": [
    "X_test_ready_SR = np.concatenate([np.ones((m_test_SR, 1)), X_norm_test_SR], axis=1)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]'\n",
    "))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}{:8.3f}'.format(\n",
    "    X_test_ready_SR[i, 0],\n",
    "    X_test_ready_SR[i, 1],\n",
    "    X_test_ready_SR[i, 2],\n",
    "    X_test_ready_SR[i, 3], \n",
    "    X_test_ready_SR[i, 4],\n",
    "    X_test_ready_SR[i, 5], \n",
    "    X_test_ready_SR[i, 6],\n",
    "    X_test_ready_SR[i, 7], \n",
    "    X_test_ready_SR[i, 8], \n",
    "    X_test_ready_SR[i, 9], \n",
    "    X_test_ready_SR[i, 10], \n",
    "    X_test_ready_SR[i, 11]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.6.3 Hacemos el calculo de Y predicha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]   X[:, 2]   X[:, 3] X[:, 4] X[:, 5] X[:, 6] X[:, 7]X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]         y     (y) usando el umbral\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   1.000  -0.643    -0.730    -1.024  -0.844  -1.589  -1.183   0.931  -0.396    -0.310  -0.238   0.497           0.20              0\n",
      "   1.000   0.393    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497           0.43              0\n",
      "   1.000   0.541    -0.730    -0.044   0.274  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497           0.44              0\n",
      "   1.000   0.097    -0.730     0.323   2.160   1.999   1.935  -0.538  -0.396    -0.310  -0.238   0.497           0.88              1\n",
      "   1.000  -0.199    -0.730    -2.982   0.204   0.803   0.896  -0.538  -0.396    -0.310  -0.238   0.497           0.66              1\n",
      "   1.000  -0.495    -0.730     0.445  -0.076   1.401   0.896  -0.538  -0.396    -0.310  -0.238  -2.014           0.73              1\n",
      "   1.000   1.281     1.370    -0.534   0.902  -0.991  -1.183  -0.538  -0.396    -0.310  -0.238   0.497           0.36              0\n",
      "   1.000  -0.643    -0.730    -1.146   1.671  -0.393  -0.143  -0.538  -0.396    -0.310  -0.238   0.497           0.42              0\n",
      "   1.000  -0.199     1.370     0.078  -0.285   0.205  -0.143  -0.538  -0.396     3.226  -0.238   0.497           0.42              0\n",
      "   1.000   0.393     1.370     0.568  -0.425   0.803   0.896   0.931  -0.396    -0.310  -0.238   0.497           0.74              1\n"
     ]
    }
   ],
   "source": [
    "y_predicha_SR =[]\n",
    "# Calculamos la Y predicha de cada fila de la matriz\n",
    "for dato in X_test_ready_SR:\n",
    "    y_predicha_SR.append(sigmoid(np.dot(dato, theta_SR.T)))\n",
    "\n",
    "# Convertimos la lista a un array unidimensional\n",
    "y_predicha_SR = np.array(y_predicha_SR)\n",
    "\n",
    "#usando el umbral donde todo aquello que sea >= 0.5 sera 1, y si es menor sera 0\n",
    "y_umbral_SR = (y_predicha_SR >= 0.5).astype(int)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}{:>10s}{:>25s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]','y','(y) usando el umbral'\n",
    "))\n",
    "print('-' * 140)\n",
    "\n",
    "#Mostrando algunos datos\n",
    "for i in range(10):\n",
    "    print('{:8.3f}{:8.3f}{:10.3f}{:10.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:8.3f}{:10.3f}{:8.3f}{:8.3f}{:15.2f}{:15.0f}'.format(\n",
    "    X_test_ready_SR[i, 0],\n",
    "    X_test_ready_SR[i, 1], \n",
    "    X_test_ready_SR[i, 2], \n",
    "    X_test_ready_SR[i, 3],\n",
    "    X_test_ready_SR[i, 4], \n",
    "    X_test_ready_SR[i, 5], \n",
    "    X_test_ready_SR[i, 6],\n",
    "    X_test_ready_SR[i, 7], \n",
    "    X_test_ready_SR[i, 8], \n",
    "    X_test_ready_SR[i, 9], \n",
    "    X_test_ready_SR[i, 10],\n",
    "    X_test_ready_SR[i, 11], \n",
    "    y_predicha_CR[i], \n",
    "    y_umbral_CR[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.6.4 Calculando la precision del entrenamiento:\n",
    "se hace uso del **np.mean**, Calcula la media (promedio) de los valores booleanos. Dado que True se interpreta como 1 y False como 0 en operaciones aritméticas, la media resultante será la proporción de elementos iguales en **y_predicha** e **y_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de entrenamiento: 72.82838 %\n"
     ]
    }
   ],
   "source": [
    "print('Precisión de entrenamiento: {:.5f} %'.format(np.mean(y_umbral_SR == y_testSR) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta parte se presentaran las conclusiones de acuerdo a las experiencias vistas en este laboratorio aplicando regularizacion y sin aplicar regularizacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cual es el mejor?\n",
    "Se comparó el desempeño del modelo de regresion logistica ya sea en el calculo del costo o tiempo de entrenamiento del modelo, incluso hasya el calculo de la presiocion del modelo, se pueden extraer varias conclusiones significativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Impacto de la regularización en la precisión del modelo:\n",
    "\n",
    " Se observa que la regularización puede tener un efecto en la precisión del modelo. En algunos casos, la regularización puede mejorar la precisión del modelo al reducir el sobreajuste (overfitting),\n",
    " especialmente cuando se tienen conjuntos de datos pequeños o altamente ruidosos. \n",
    " \n",
    " Sin embargo, en conjuntos de datos más grandes o con menos ruido, la regularización puede no proporcionar mejoras significativas en la precisión y podría incluso degradarla.\n",
    "\n",
    " Para este caso se hizo la comparacion entre la preciscion de ambos modelos, con y sin regularizacion:\n",
    "\n",
    "| Modelo  | Precision del modelo con regularizacion| Presicion del modelo sin regularizacion| Dieferencia entre ambos|\n",
    "|---------|------|---------|-----------|\n",
    "| Regresion Logistica | 72.89380 %  | 72.82838 % | 0.06542 % |\n",
    "\n",
    "Se puede notar que la diferencia entre ambos es poca, pero se puede notar que la precision del modelo con regularizar es un poco mas alta que su contraparte, pero esto nos da una idea que que al aplicar regularizacion la presicion llega a aumentar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Calculo del costo\n",
    "\n",
    "Al agregar un término de penalización a la función de costo, la regularización restringe los coeficientes del modelo, lo que evita que se vuelvan demasiado grandes y sensibles a pequeñas variaciones en los datos de entrenamiento y de acuerdo a esto se hizo una comparacion de los costos obtenidos para amobos modelos:\n",
    "\n",
    "| Modelo  | Costo calculado del modelo con regularizacion | Costo calculado del modelo sin regularizacion |Diferencia entre ambos|\n",
    "|---------|------|---------|-----------|\n",
    "| Regresion Logistica| 0.572162951901662 | 0.5647883697613265 | 0.007374582140335499 |\n",
    "\n",
    "Como se puede observar, la diferencia entre los costos es muy pequeña. El costo del modelo aplicado sin regularización es menor que el del modelo aplicado con regularización. Esto se debe a que al aplicar regularización se incrementó el hiperparámetro lambda, lo que elevó el valor del costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Tiempo de entrenamiento\n",
    "\n",
    "En algunos casos, la regularización puede aumentar el tiempo de entrenamiento del modelo debido a la necesidad de calcular los términos adicionales de penalización en la función de costo y actualizar los coeficientes del modelo de manera iterativa.\n",
    "\n",
    "Haciendo una tabla para ver los cambios en el tiempo de entranamiento de entrenamiento:\n",
    "\n",
    "| Modelo  | Tiempo de entrenamiento del modelo con regularizacion | Tiempo de entrenamiento del modelo sin regularizacion |Dieferencia entre ambos|\n",
    "|---------|------|---------|-----------|\n",
    "| Regresion Logistica | 31.4 seg   | 30.1 seg  | 1.3 seg |\n",
    "\n",
    "Como se puede observar el tiempo de entrenamiento para el modelo usando regularizacion es con pocos segundos mayor con respecto al usado sin regularizacion. Puede que la diferencia no sea mucha, e incluso aveces no se podria notar, pero aun asi se nota una diferencia cuando se aplica el hiperparametro lambda para la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Entonces cual es el mejor?\n",
    "\n",
    "Una vez hecha las comparaciones del desempeño de modelos de con regularizacion y sin regularización en las partes de costo, tiempo de entrenamiento y precision de ambos modelos, se puede determino que el mejor modelo es aquel que se aplica regularizacion.\n",
    "\n",
    "**¿Porque?**\n",
    "\n",
    "Porque respecto al costo y el tiempo de entrenamiento aunque estos tenga una diferencia casi insignificante, la caracteristica que mas se tomo en cuenta la precision del modelo, ya que aunque la diferencia entre estos es muy minima, mientras el modelo este mas cercano al 100% de su precision es mucho mejor, porque si tenemos un modelo con mas ejemplos puede que la precision de este mejore considerablemente al aplicar regularizacion, y la diferencia entre estos ya no sea una cercana a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuales son los mejores hiperparametros?\n",
    "\n",
    "Se eligio aquellos hiperparametros de acuerdo a la grafica de costo realizada para cada modelo, guiandonos en que la grafica de costo no este muy curveada pero tampoco que este demasiado recta. A continuacion se muestra los hiperparametros seleccionados:\n",
    "\n",
    "- **Numero de iteraciones.-** se eligio como una cantidad de **15000** iteraciones, esto para evitar el sobreajuste en la grafica de costo, guiandonos que la grafica de costo no se curvee a un numero mayor de iteraciones a 15000 o llegue a formar una recta cuanto este era demasiado menor a 15000 numero de iteraciones.\n",
    "\n",
    "- **alpha.-** Para este hiperparametro se selecciono un valor de **0.0009** para ambos modelos, esto porque se queria buscar una precision muy cercana al valor que se deseaba predecir, ya que a menor valor de alpha, se llega a tener mayor precision del modelo, pero esto lleva a un mayor consumo de recursos al equipo de computo, ya que a menor valor de alpha el tiempo de entrenamiento era mucho mas alto y el equipo de computo llegaba a tener sobrecalentarse.\n",
    "\n",
    "- **Lambda.-** Para el hiperparametro **lambda** se uso el valor de **1000**, igualmente se siguio las recomendaciones que el hiperparametro lambda podria estar entre 10, 100 o 1000, se hizo la prueba con cada uno pero el que mejor resultados pero con mayor tiempo de entrenamiento y procesamiento nos dio fue el valor de un lamdda de *1000*, pero se hizo la prueba con un valor de lambda cercano a cero, y nos genero un costo muy cercano al costo del modelo sin regularizar, mientras que si usabamos un valor lambda negativo nos generaba una grafica contraria al de la curva esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cual asumirias en una determinada circunstancia?\n",
    "\n",
    "El modelo aplicado con regularizacion seria mejor aplicado en un circunstancia en donde usemos un numero de iteraciones demasiado alto, en este caso 15000, esto para evitar el overfiting, pero sin olvidar que se requeriria un equipo de computo con buenas caracteristicas para que no haya problemas en la fase de entrenamiento. O tambien un caso donde tengamos una cantidad alta de ejemplos e igualmente vayamos a usar un numero de iteraciones demasiado alto.\n",
    "\n",
    "El modelo aplicado sin regularizacion seria mejor aplicado a una circunstancia en donde se tenga pocos parametros, ya que modelos más complejos, como aquellos con una gran cantidad de características o parámetros, tienden a ser más propensos al overfiting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
